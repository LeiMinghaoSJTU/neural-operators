{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " # 训练状态的断点保存与加载 (Checkpointing and loading training states)\n",
    "\n",
    "\n",
    "\n",
    " 本示例演示NeuralOP库中``Trainer``类的保存和加载功能，\n",
    "\n",
    " 该功能可以轻松实现训练状态的断点保存（checkpoint）和恢复训练（resume training）。\n",
    "\n",
    "\n",
    "\n",
    " ## 导入依赖库\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch核心库，用于张量计算和深度学习\n",
    "import torch\n",
    "# 导入matplotlib绘图库（本示例未实际使用，为预留可视化接口）\n",
    "import matplotlib.pyplot as plt\n",
    "# 导入系统库，用于刷新输出流\n",
    "import sys\n",
    "# 从neuralop.models导入FNO（Fourier Neural Operator）模型，这是神经算子的核心模型\n",
    "from neuralop.models import FNO\n",
    "# 从neuralop导入Trainer训练器，用于封装训练流程\n",
    "from neuralop import Trainer\n",
    "# 从neuralop.training导入AdamW优化器（带权重衰减的Adam）\n",
    "from neuralop.training import AdamW\n",
    "# 从neuralop数据模块加载Darcy Flow（达西流）小型数据集，这是流体力学的经典基准数据集\n",
    "from neuralop.data.datasets import load_darcy_flow_small\n",
    "# 导入参数计数工具，用于统计模型参数量\n",
    "from neuralop.utils import count_model_params\n",
    "# 导入损失函数：LpLoss（Lp范数损失）和H1Loss（H1范数损失，包含函数值和梯度的损失）\n",
    "from neuralop import LpLoss, H1Loss\n",
    "\n",
    "# 设置训练设备，cpu表示使用CPU训练（若有GPU可改为\"cuda\"）\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## 加载Darcy-Flow（达西流）数据集\n",
    "\n",
    " 达西流数据集是用于测试神经算子性能的经典流体力学数据集，\n",
    "\n",
    " 输入为渗透率场，输出为压力场\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db for resolution 16 with 100 samples \n",
      "Loading test db for resolution 32 with 50 samples \n"
     ]
    }
   ],
   "source": [
    "# 加载小型达西流数据集，并返回训练加载器、测试加载器和数据处理器\n",
    "train_loader, test_loaders, data_processor = load_darcy_flow_small(\n",
    "    n_train=1000,          # 训练集样本数量：1000个\n",
    "    batch_size=32,         # 训练批次大小：每次迭代处理32个样本\n",
    "    test_resolutions=[16, 32],  # 测试集数据的分辨率：16x16和32x32\n",
    "    n_tests=[100, 50],     # 对应不同分辨率测试集的样本数：16x16有100个，32x32有50个\n",
    "    test_batch_sizes=[32, 32],  # 测试集批次大小：两种分辨率均为32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " ## 创建FNO（Fourier Neural Operator）模型\n",
    "\n",
    " 傅里叶神经算子是基于傅里叶变换的神经算子，适用于偏微分方程求解等场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 520761 parameters.\n"
     ]
    }
   ],
   "source": [
    "# 初始化FNO模型\n",
    "model = FNO(\n",
    "    n_modes=(16, 16),          # 傅里叶模态数：在x和y方向各保留16个模态（模态数越多，拟合能力越强，但计算量越大）\n",
    "    in_channels=1,             # 输入通道数：1（达西流的渗透率场为单通道）\n",
    "    out_channels=1,            # 输出通道数：1（达西流的压力场为单通道）\n",
    "    hidden_channels=32,        # 隐藏层通道数：32（控制模型宽度，影响容量）\n",
    "    projection_channel_ratio=2, # 投影通道比例：2（用于控制投影层的通道数）\n",
    "    factorization=\"tucker\",    # 张量分解方式：tucker分解（减少参数量，防止过拟合）\n",
    "    rank=0.42,                 # 分解秩：0.42（秩越小，参数量越少，分解越激进）\n",
    ")\n",
    "\n",
    "# 将模型移到指定设备（CPU/GPU）\n",
    "model = model.to(device)\n",
    "\n",
    "# 统计模型参数量\n",
    "n_params = count_model_params(model)\n",
    "# 打印参数量信息\n",
    "print(f\"\\nOur model has {n_params} parameters.\")\n",
    "# 强制刷新标准输出，确保信息即时打印\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " ## 创建优化器和学习率调度器\n",
    "\n",
    " 优化器用于更新模型参数，调度器用于动态调整学习率\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化AdamW优化器（带权重衰减的Adam，防止过拟合）\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),    # 待优化的模型参数\n",
    "    lr=8e-3,               # 初始学习率：0.008\n",
    "    weight_decay=1e-4      # 权重衰减系数：0.0001（L2正则化，防止过拟合）\n",
    ")\n",
    "\n",
    "# 初始化余弦退火学习率调度器\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,     # 关联的优化器\n",
    "    T_max=30       # 学习率周期：30个epoch（学习率在30个epoch内从初始值余弦衰减到0）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " ## 创建损失函数\n",
    "\n",
    " 损失函数用于衡量模型预测值与真实值的差距，指导模型优化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化L2损失（LpLoss中d=2表示2维数据，p=2即为L2范数）\n",
    "l2loss = LpLoss(d=2, p=2)\n",
    "# 初始化H1损失（包含函数值的L2损失 + 梯度的L2损失，更严格的损失度量）\n",
    "h1loss = H1Loss(d=2)\n",
    "\n",
    "# 训练损失选择H1Loss（更适合偏微分方程的求解任务）\n",
    "train_loss = h1loss\n",
    "# 定义评估损失字典：包含H1和L2两种损失，用于测试阶段评估\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " ## 打印训练配置信息\n",
    "\n",
    " 确认模型、优化器、调度器和损失函数的配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### OPTIMIZER ###\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001C829C053A0>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x000001C82B78EAE0>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x000001C82B78EAE0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x000001C82B78E660>}\n"
     ]
    }
   ],
   "source": [
    "# 打印优化器配置\n",
    "print(\"\\n### OPTIMIZER ###\\n\", optimizer)\n",
    "# 打印学习率调度器配置\n",
    "print(\"\\n### SCHEDULER ###\\n\", scheduler)\n",
    "# 打印损失函数配置\n",
    "print(\"\\n### LOSSES ###\")\n",
    "print(f\"\\n * Train: {train_loss}\")       # 训练损失\n",
    "print(f\"\\n * Test: {eval_losses}\")       # 测试损失\n",
    "# 强制刷新输出\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " ## 创建Trainer训练器实例\n",
    "\n",
    " Trainer是NeuralOP封装的训练管理器，简化训练流程的实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                # 待训练的模型\n",
    "    n_epochs=20,                # 总训练轮数：20个epoch\n",
    "    device=device,              # 训练设备\n",
    "    data_processor=data_processor,  # 数据处理器（用于数据的预处理/后处理）\n",
    "    wandb_log=False,            # 关闭Weights & Biases日志记录（可视化工具）\n",
    "    eval_interval=3,            # 评估间隔：每3个epoch进行一次测试集评估\n",
    "    use_distributed=False,      # 关闭分布式训练（单机训练）\n",
    "    verbose=True,               # 开启详细日志输出（打印训练过程信息）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    " ## 训练模型\n",
    "\n",
    " 第一阶段：正常训练并保存断点（checkpoint）\n",
    "\n",
    " 第二阶段：从保存的断点恢复训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1000 samples\n",
      "Testing on [] samples         on resolutions [].\n",
      "Raw outputs of shape torch.Size([32, 1, 16, 16])\n",
      "[0] time=5.63, avg_loss=0.6552, train_err=20.4755\n",
      "Eval: \n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[3] time=5.09, avg_loss=0.2278, train_err=7.1179\n",
      "Eval: \n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[6] time=5.15, avg_loss=0.1928, train_err=6.0261\n",
      "Eval: \n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[9] time=5.08, avg_loss=0.1652, train_err=5.1619\n",
      "Eval: \n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[12] time=6.08, avg_loss=0.1646, train_err=5.1448\n",
      "Eval: \n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[15] time=5.01, avg_loss=0.1448, train_err=4.5252\n",
      "Eval: \n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[18] time=5.00, avg_loss=0.1440, train_err=4.4988\n",
      "Eval: \n",
      "[Rank 0]: saved training state to ./checkpoints\n",
      "[Rank 0]: saved training state to ./checkpoints\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch._C._nn.gelu was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch._C._nn.gelu])` or the `torch.serialization.safe_globals([torch._C._nn.gelu])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     15\u001b[39m trainer = Trainer(\n\u001b[32m     16\u001b[39m     model=model,\n\u001b[32m     17\u001b[39m     n_epochs=\u001b[32m20\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 从断点目录恢复训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./checkpoints\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 断点恢复目录：读取之前保存的checkpoints文件夹\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\neuralop\\training\\trainer.py:198\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_loader, test_loaders, optimizer, scheduler, regularizer, training_loss, eval_losses, eval_modes, save_every, save_best, save_dir, resume_from_dir, max_autoregressive_steps)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28mself\u001b[39m.save_best = save_best\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resume_from_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresume_state_from_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Load model and data_processor to device\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.model.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\neuralop\\training\\trainer.py:781\u001b[39m, in \u001b[36mTrainer.resume_state_from_dir\u001b[39m\u001b[34m(self, save_dir)\u001b[39m\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    770\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError: resume_from_dir expects a model\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m    771\u001b[39m \u001b[33m                                state dict named model.pt or best_model.pt.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    772\u001b[39m     )\n\u001b[32m    773\u001b[39m \u001b[38;5;66;03m# returns model, loads other modules if provided\u001b[39;00m\n\u001b[32m    775\u001b[39m (\n\u001b[32m    776\u001b[39m     \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer,\n\u001b[32m    778\u001b[39m     \u001b[38;5;28mself\u001b[39m.scheduler,\n\u001b[32m    779\u001b[39m     \u001b[38;5;28mself\u001b[39m.regularizer,\n\u001b[32m    780\u001b[39m     resume_epoch,\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m ) = \u001b[43mload_training_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resume_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resume_epoch > \u001b[38;5;28mself\u001b[39m.start_epoch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\neuralop\\training\\training_state.py:78\u001b[39m, in \u001b[36mload_training_state\u001b[39m\u001b[34m(save_dir, save_name, model, optimizer, scheduler, regularizer, map_location)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m     save_pth = save_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_state_dict.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_pth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# load optimizer if state exists\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch._C._nn.gelu was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch._C._nn.gelu])` or the `torch.serialization.safe_globals([torch._C._nn.gelu])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# 第一阶段训练：训练并保存断点\n",
    "trainer.train(\n",
    "    train_loader=train_loader,  # 训练数据加载器\n",
    "    test_loaders={},            # 测试数据加载器（本示例暂不使用，传空字典）\n",
    "    optimizer=optimizer,        # 优化器\n",
    "    scheduler=scheduler,        # 学习率调度器\n",
    "    regularizer=False,          # 关闭正则化\n",
    "    training_loss=train_loss,   # 训练损失函数\n",
    "    save_every=1,               # 保存间隔：每1个epoch保存一次断点\n",
    "    save_dir=\"./checkpoints\",   # 断点保存目录：当前路径下的checkpoints文件夹\n",
    ")\n",
    "\n",
    "# 第二阶段：从保存的断点（第10个epoch）恢复训练\n",
    "# 重新创建Trainer实例（也可复用原实例，此处为演示完整流程）\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    n_epochs=20,\n",
    "    device=device,\n",
    "    data_processor=data_processor,\n",
    "    wandb_log=False,\n",
    "    eval_interval=3,\n",
    "    use_distributed=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 从断点目录恢复训练\n",
    "trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    test_loaders={},\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    regularizer=False,\n",
    "    training_loss=train_loss,\n",
    "    resume_from_dir=\"./checkpoints\",  # 断点恢复目录：读取之前保存的checkpoints文件夹\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
