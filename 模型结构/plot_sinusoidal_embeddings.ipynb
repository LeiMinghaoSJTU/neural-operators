{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Sinusoidal Embeddings\n",
        "\n",
        "Inputs to deep learning models often represent positions on a spatial, temporal, or \n",
        "spatio-temporal grid. To enrich these coordinates, positional embeddings can be introduced \n",
        "to improve a model's capacity to generalize across the domain. In this tutorial, we focus \n",
        "on sinusoidal positional embeddings.\n",
        "\n",
        "Sinusoidal embeddings encode inputs as periodic functions (sines and cosines), thereby \n",
        "lifting low-dimensional coordinates into a richer spectral representation. This spectral \n",
        "lifting enhances the model's ability to capture fine-scale variations and high-frequency \n",
        "dynamics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Setup in 1D\n",
        "To build intuition, consider a simple 1D example. Let $x \\in \\mathbb{R}$\n",
        "be a single input, and define the embedding function\n",
        "\n",
        "$$g: \\mathbb{R} \\rightarrow \\mathbb{R}^{2 L}, \\quad g(x)=[\\sin (x), \\  \\cos (x), \\ \\sin (2 x), \\ \\cos (2 x), \\ldots, \\ \\sin (L x), \\ \\cos (L x)],$$\n",
        "\n",
        "where $L$ defines the number of frequencies we wish to use for the embedding. Each\n",
        "pair of sine and cosine terms introduces a higher frequency, enriching how positional\n",
        "information is represented.\n",
        "\n",
        "This idea naturally extends to an entire 1D input. Let $\\vec{x} \\in \\mathbb{R}^N$\n",
        "denote a discretized domain of $N$ points. Then the embedding function becomes\n",
        "\n",
        "$$g: \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times 2 L}, \\quad g(\\vec{x})=\\operatorname{concat}(\\sin (\\vec{x}), \\cos (\\vec{x}), \\sin (2 \\vec{x}), \\cos (2 \\vec{x}), \\ldots, \\sin (L \\vec{x}), \\cos (L \\vec{x})),$$\n",
        "\n",
        "In practice, both the original coordinate and its embedding are passed to the model:\n",
        "\n",
        "$$\\operatorname{input}(\\vec{x})=\\operatorname{concat}(\\vec{x}, \\ g(\\vec{x})) \\in \\mathbb{R}^{N \\times 2 L + 1},$$\n",
        "\n",
        "preserving the original input, while augmenting it with a hierarchy of frequency components.\n",
        "\n",
        "\n",
        "\n",
        "### Domain Normalization\n",
        "When applying sinusoidal embeddings, it is often useful to normalize the input coordinates\n",
        "to a periodic interval that aligns with the natural period of the sine and cosine functions.\n",
        "For example, a 1D spatial domain $\\vec{x} \\in[0,1]$ of $N$ points can be rescaled to\n",
        "\n",
        "$$\\vec{x}^{\\prime}=2 \\pi \\vec{x},$$\n",
        "\n",
        "so that $\\vec{x}^{\\prime} \\in[0,2 \\pi]$.\n",
        "\n",
        "This mapping preserves the number of sampling points $N$ and the overall shape of the domain\n",
        "while ensuring that the lowest-frequency sine and cosine components complete exactly one\n",
        "full oscillation over the interval.\n",
        "\n",
        "\n",
        "### Choosing $L$ to Satisfy the Nyquist-Criterion\n",
        "\n",
        "> **Warning**    \n",
        "> When choosing the number of frequency levels $L$, it is important to ensure that the\n",
        "   highest frequency component in the embedding does not exceed the Nyquist limit imposed by\n",
        "   the discretisation of the input domain.\n",
        "\n",
        "For a domain of $N$ points, the Nyquist frequency is\n",
        "\n",
        "$$f_{\\text{Nyquist}} = \\frac{N}{2}.$$\n",
        "\n",
        "For the sinusoidal embedding defined above, the Nyquist constraint becomes:\n",
        "\n",
        "$$L < \\frac{N}{2}.$$\n",
        "\n",
        "The Nyquist frequency represents the maximum frequency that can be correctly captured\n",
        "when sampling a signal, equal to half the sampling rate. If frequencies higher than this\n",
        "limit are used, they will not be represented as true high frequencies but will instead appear\n",
        "as lower ones, producing distortion known as aliasing. This is why we must ensure that\n",
        "the highest frequency in our embedding does not exceed the Nyquist limit.\n",
        "\n",
        "\n",
        "### Visualizing the Sinusoidal Embeddings\n",
        "Below, we visualize the sinusoidal embeddings for a spatial input domain\n",
        "$\\vec{x} \\in[0,1]$ consisting of 1000 equally spaced points, using $L = 3$ frequency levels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from neuralop.layers.embeddings import SinusoidalEmbedding\n",
        "\n",
        "# Set default font sizes for better readability\n",
        "plt.rcParams.update(\n",
        "    {\n",
        "        \"font.size\": 14,\n",
        "        \"axes.titlesize\": 18,\n",
        "        \"axes.labelsize\": 16,\n",
        "        \"xtick.labelsize\": 14,\n",
        "        \"ytick.labelsize\": 14,\n",
        "        \"legend.fontsize\": 14,\n",
        "    }\n",
        ")\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "# Define a spatial domain and number of frequencies\n",
        "# Create 1000 equally spaced points in [0, 1]\n",
        "#  and normalize to [0, 2π] for proper sinusoidal embedding\n",
        "x = torch.linspace(0, 1, 1000)\n",
        "x_normalized = torch.linspace(0, 2 * torch.pi, len(x))\n",
        "# Number of frequency levels for the embedding\n",
        "L = 3\n",
        "\n",
        "# Check if the number of frequencies satisfies the Nyquist-Criterion\n",
        "if L < len(x_normalized)/2:\n",
        "    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\n",
        "else:\n",
        "    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n",
        "\n",
        "# Build embedding: [sin(x), cos(x), sin(2x), cos(2x), ...]\n",
        "# Each frequency level contributes a sine and cosine pair\n",
        "g = []\n",
        "for l in range(1, L + 1):\n",
        "    g.append(torch.sin(l * x_normalized))\n",
        "    g.append(torch.cos(l * x_normalized))\n",
        "\n",
        "# Construct input by concatenating the original input and the embedding\n",
        "# This preserves the original coordinates while adding spectral information\n",
        "input_arr = np.asarray([x, *g])\n",
        "input_tensor = torch.tensor(input_arr)\n",
        "\n",
        "# Plot the embedding components\n",
        "colors = plt.cm.tab10.colors\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for freq_idx in range(L):\n",
        "    color = colors[freq_idx % len(colors)]\n",
        "    sin_idx = 2 * freq_idx + 1\n",
        "    cos_idx = 2 * freq_idx + 2\n",
        "\n",
        "    plt.plot(x, input_tensor[sin_idx], color=color, label=f\"Frequency {freq_idx + 1}\")\n",
        "    plt.plot(x, input_tensor[cos_idx], color=color)\n",
        "\n",
        "plt.xlabel(\"x\", fontsize=16)\n",
        "plt.ylabel(\"Embedding value\", fontsize=16)\n",
        "plt.title(\"Sinusoidal Embedding Components (L = 3)\", fontsize=18)\n",
        "plt.legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\n",
        "plt.locator_params(axis=\"y\", nbins=5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Encoding Constant Parameters\n",
        "A particularly useful extension of sinusoidal embeddings is their ability to encode constant\n",
        "parameters. Consider a setting where you have a scalar parameter $m$ (such as a material\n",
        "property, boundary condition, or physical constant) that you wish to feed into a model.\n",
        "Instead of treating $m$ as a fixed scalar input, we can represent it using periodic\n",
        "functions, either by modulating the amplitude or the frequency of the sinusoidal components.\n",
        "\n",
        "\n",
        "**1. Amplitude Modulation:** To encode $m$ by scaling the amplitudes of the sinusoidal\n",
        "functions, we define the embedding as\n",
        "\n",
        "$$m \\rightarrow m g(\\vec{x}),$$\n",
        "\n",
        "where each element of the embedding $g(\\vec{x})$ is multiplied by $m$.\n",
        "\n",
        "\n",
        "**2. Frequency Modulation:** Alternatively, to encode $m$ by scaling the frequencies,\n",
        "we define\n",
        "\n",
        "$$m \\rightarrow g(m\\vec{x})$$\n",
        "\n",
        "where $m$ multiplies the input argument of each sinusoidal component.\n",
        "\n",
        "When encoding constant parameters through frequency modulation, care must be taken to ensure\n",
        "that the Nyquist criterion is satisfied. In this case, where the modulation factor $m$\n",
        "scales the frequencies, the Nyquist constraint becomes $L < \\frac{N}{2m}$.\n",
        "\n",
        "\n",
        "Below, we demonstrate an example of encoding the parameter $m = 2.5$ through both\n",
        "amplitude and frequency modulation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and number of frequencies\n",
        "x = torch.linspace(0, 1, 1000)\n",
        "x_normalized = torch.linspace(0, 2 * torch.pi, len(x))\n",
        "L = 3\n",
        "\n",
        "# Define parameter to encode\n",
        "m = 2.5\n",
        "m_tensor = torch.tensor([m])\n",
        "\n",
        "# Check if the number of frequencies and parameter satisfies the Nyquist-Criterion\n",
        "if L <= len(x_normalized)/(2 * m):\n",
        "    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given parameter {m} and number of frequencies {L}.\")\n",
        "else:\n",
        "    print(f\"Nyquist-Shannon sampling theorem is violated for the given parameter {m} and number of frequencies {L}.\")\n",
        "\n",
        "# Build amplitude-modulated embedding: m * g(x)\n",
        "g_amplitude = []\n",
        "for l in range(1, L + 1):\n",
        "    g_amplitude.append(torch.sin(l * x_normalized) * m_tensor)\n",
        "    g_amplitude.append(torch.cos(l * x_normalized) * m_tensor)\n",
        "\n",
        "# Build frequency-modulated embedding: g(m * x)\n",
        "g_frequency = []\n",
        "for l in range(1, L + 1):\n",
        "    g_frequency.append(torch.sin(l * x_normalized * m_tensor))\n",
        "    g_frequency.append(torch.cos(l * x_normalized * m_tensor))\n",
        "\n",
        "# Convert to arrays for visualization\n",
        "input_amplitude = torch.tensor(np.asarray([x, *g_amplitude]))\n",
        "input_frequency = torch.tensor(np.asarray([x, *g_frequency]))\n",
        "\n",
        "# Plot both embeddings\n",
        "colors = plt.cm.tab10.colors\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 9), sharex=True)\n",
        "\n",
        "## Amplitude modulation\n",
        "for freq_idx in range(L):\n",
        "    color = colors[freq_idx % len(colors)]\n",
        "    sin_idx, cos_idx = 2 * freq_idx + 1, 2 * freq_idx + 2\n",
        "    axes[0].plot(x, input_amplitude[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n",
        "    axes[0].plot(x, input_amplitude[cos_idx], color=color)\n",
        "axes[0].set_title(\"Amplitude Modulation\", fontsize=18, pad=20)\n",
        "axes[0].set_ylabel(\"Embedding value\", fontsize=16)\n",
        "axes[0].legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\n",
        "axes[0].locator_params(axis=\"y\", nbins=5)\n",
        "\n",
        "## Frequency modulation\n",
        "for freq_idx in range(L):\n",
        "    color = colors[freq_idx % len(colors)]\n",
        "    sin_idx, cos_idx = 2 * freq_idx + 1, 2 * freq_idx + 2\n",
        "    axes[1].plot(x, input_frequency[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n",
        "    axes[1].plot(x, input_frequency[cos_idx], color=color)\n",
        "axes[1].set_title(\"Frequency Modulation\", fontsize=18, pad=20)\n",
        "axes[1].set_ylabel(\"Embedding value\", fontsize=16)\n",
        "axes[1].set_xlabel(\"x\", fontsize=16)\n",
        "axes[1].locator_params(axis=\"y\", nbins=5)\n",
        "\n",
        "plt.suptitle(f\"Sinusoidal Embeddings with Parameter m = {m}\", y=0.98, fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Neural Operator SinusoidalEmbedding Class\n",
        "The ``neuralop`` library provides a unified sinusoidal positional embedding class,\n",
        "``neuralop.layers.embeddings.SinusoidalEmbedding``, with the following embedding techniques:\n",
        "\n",
        "- ``transformer`` - Vaswani, A. et al (2017), \"Attention Is All You Need\".\n",
        "- ``nerf`` - Mildenhall, B. et al (2020), \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\".\n",
        "\n",
        "The `SinusoidalEmbedding` class expects inputs to be of shape\n",
        "\n",
        "             ``(batch_size, N, input_channels)`` or ``(N, input_channels)``\n",
        "\n",
        "\n",
        "### Embedding Variants\n",
        "Let $\\vec{x} \\in \\mathbb{R}^N$ denote a 1D input domain consisting of\n",
        "$N$ discretized points. The embedding function\n",
        "$g: \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times 2L}$ maps each input value\n",
        "$x_n$ to a $2L$-dimensional vector composed of sine and cosine terms evaluated\n",
        "at different frequencies. Each embedding type defines these frequencies differently,\n",
        "leading to distinct representations.\n",
        "\n",
        "\n",
        "\n",
        "**1. Transformer-style embedding:** For $0 \\leq k < L$:\n",
        "\n",
        "$$g(\\vec{x})_{:, 2 k}=\\sin \\left(\\frac{\\vec{x}}{\\text { max_positions }^{k / L}}\\right), \\quad g(\\vec{x})_{:, 2 k+1}=\\cos \\left(\\frac{\\vec{x}}{\\text { max_positions }^{k / L}}\\right) .$$\n",
        "\n",
        "Here, $\\text{max_positions}$ controls the maximum position for the embedding.\n",
        "\n",
        "\n",
        "**2. NeRF-style embedding:** For $0 \\leq k < L$:\n",
        "\n",
        "$$g(\\vec{x})_{:, 2 k}=\\sin \\left(2^k \\pi \\vec{x}\\right), \\quad g(\\vec{x})_{:, 2 k+1}=\\cos \\left(2^k \\pi \\vec{x}\\right) .$$\n",
        "\n",
        "In order to ensure that the Nyquist-Criterion is satisfied, for the Transformer-style\n",
        "embedding, the embedding frequencies should satisfy: $f_{\\max} < f_{\\text{Nyquist}}$.\n",
        "\n",
        "For the NeRF-style embedding:\n",
        "\n",
        "$$2^{L-1} < \\frac{N}{2} \\ \\ \\implies \\ \\  L < 1 + \\log_2\\left(\\frac{N}{2}\\right).$$\n",
        "\n",
        "\n",
        "Below, we include examples of using the `SinusoidalEmbedding` class with both the\n",
        "transformer- and NeRF-style embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and the number of frequencies\n",
        "x = torch.linspace(0, 1, 1000)\n",
        "x_normalized = torch.linspace(0, 2 * torch.pi, len(x)).reshape(-1, 1)\n",
        "L = 3\n",
        "\n",
        "# Check if the number of frequencies satisfies the Nyquist-Criterion\n",
        "if L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2 )):\n",
        "    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\n",
        "else:\n",
        "    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n",
        "\n",
        "# Define the transformer embedding\n",
        "# max_positions controls the frequency scaling in transformer-style embeddings\n",
        "max_positions = 1000\n",
        "transformer_embedder = SinusoidalEmbedding(\n",
        "    in_channels=1,\n",
        "    num_frequencies=L,\n",
        "    embedding_type=\"transformer\",\n",
        "    max_positions=max_positions,\n",
        ").to(device)\n",
        "\n",
        "# Apply transformer-style embedding\n",
        "transformer_embedding = transformer_embedder(x_normalized).permute(1, 0)\n",
        "\n",
        "# Define the NeRF embedding\n",
        "nerf_embedder = SinusoidalEmbedding(\n",
        "    in_channels=1, num_frequencies=L, embedding_type=\"nerf\"\n",
        ").to(device)\n",
        "\n",
        "# Apply NeRF-style embedding\n",
        "nerf_embedding = nerf_embedder(x_normalized).permute(1, 0)\n",
        "\n",
        "# Plot both embeddings\n",
        "colors = plt.cm.tab10.colors\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 9), sharex=True)\n",
        "\n",
        "## Transformer embedding\n",
        "for freq_idx in range(L):\n",
        "    color = colors[freq_idx % len(colors)]\n",
        "    sin_idx, cos_idx = 2 * freq_idx, 2 * freq_idx + 1\n",
        "\n",
        "    axes[0].plot(x, transformer_embedding[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n",
        "    axes[0].plot(x, transformer_embedding[cos_idx], color=color)\n",
        "\n",
        "axes[0].set_title(\"Transformer embedding\", fontsize=18, pad=20)\n",
        "axes[0].set_ylabel(\"Embedding value\", fontsize=16)\n",
        "axes[0].legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\n",
        "axes[0].locator_params(axis=\"y\", nbins=5)\n",
        "\n",
        "## NeRF embedding\n",
        "for freq_idx in range(L):\n",
        "    color = colors[freq_idx % len(colors)]\n",
        "    sin_idx, cos_idx = 2 * freq_idx, 2 * freq_idx + 1\n",
        "\n",
        "    axes[1].plot(x, nerf_embedding[sin_idx], color=color, label=f\"Channel {freq_idx + 1}\")\n",
        "    axes[1].plot(x, nerf_embedding[cos_idx], color=color)\n",
        "\n",
        "axes[1].set_title(\"NeRF embedding\", fontsize=18, pad=20)\n",
        "axes[1].set_xlabel(\"x\", fontsize=16)\n",
        "axes[1].set_ylabel(\"Embedding value\", fontsize=16)\n",
        "axes[1].locator_params(axis=\"y\", nbins=5)\n",
        "\n",
        "plt.suptitle(\"Sinusoidal Embeddings using transformer and NeRF embedding types\", y=0.98, fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Encoding Constant Parameters with NeuralOp Class\n",
        "Similar to the earlier illustrative examples, we can also encode\n",
        "a scalar parameter $m$ before passing it to a model. Once again, care must be taken to\n",
        "ensure that the Nyquist criterion is satisfied.\n",
        "\n",
        "In the Transformer-style embedding, to avoid aliasing, the embedding frequencies should still\n",
        "satisfy\n",
        "\n",
        "$$f_{\\max} < f_{\\text{Nyquist}}.$$\n",
        "\n",
        "For the NeRF-style embedding, the modified constraint becomes:\n",
        "\n",
        "$$2^{L-1}m < \\frac{N}{2} \\implies L < 1 + \\log_2\\left(\\frac{N}{2m}\\right).$$\n",
        "\n",
        "\n",
        "\n",
        "Below, we demonstrate an example of encoding the parameter $m = 2.5$ through frequency\n",
        "modulation of the NeRF-style embedding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and the number of frequencies\n",
        "x = torch.linspace(0, 1, 1000)\n",
        "x_normalized = torch.linspace(0, 2 * torch.pi, len(x)).reshape(-1, 1)\n",
        "L = 3\n",
        "\n",
        "# Define the parameter to encode\n",
        "m = 2.5\n",
        "m_tensor = torch.tensor([m])\n",
        "\n",
        "# Check if the number of frequencies and parameter satisfies the Nyquist-Criterion\n",
        "if L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2 * m)):\n",
        "    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given parameter {m} and number of frequencies {L}.\")\n",
        "else:\n",
        "    print(f\"Nyquist-Shannon sampling theorem is violated for the given parameter {m} and number of frequencies {L}.\")\n",
        "\n",
        "# Define the NeRF embedding\n",
        "nerf_embedder = SinusoidalEmbedding(\n",
        "    in_channels=1, num_frequencies=L, embedding_type=\"nerf\"\n",
        ").to(device)\n",
        "\n",
        "# Apply frequency modulation: multiply input by parameter before embedding\n",
        "# This scales all frequencies by the parameter m\n",
        "nerf_embedding = nerf_embedder(x_normalized * m_tensor).permute(1, 0)\n",
        "\n",
        "# Plot the embedding\n",
        "colors = plt.cm.tab10.colors\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for freq_idx in range(L):\n",
        "    color = colors[freq_idx % len(colors)]\n",
        "    sin_idx = 2 * freq_idx\n",
        "    cos_idx = 2 * freq_idx + 1\n",
        "\n",
        "    plt.plot(x, nerf_embedding[sin_idx], color=color, label=f\"Channel {freq_idx}\")\n",
        "    plt.plot(x, nerf_embedding[cos_idx], color=color)\n",
        "\n",
        "plt.xlabel(\"x\", fontsize=16)\n",
        "plt.ylabel(\"Embedding\", fontsize=16)\n",
        "plt.title(\"NeRF-style embedding with modulated frequency\", fontsize=18, pad=20)\n",
        "plt.legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\n",
        "plt.locator_params(axis=\"y\", nbins=5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, we can encode the parameter $m = 2.5$ through amplitude modulation, where we show\n",
        "an example using the NeRF-style embedding below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a spatial domain and the number of frequencies\n",
        "x = torch.linspace(0, 1, 1000)\n",
        "x_normalized = torch.linspace(0, 2 * torch.pi, len(x)).reshape(-1, 1)\n",
        "L = 3\n",
        "\n",
        "# Define the parameter to encode\n",
        "m = 2.5\n",
        "m_tensor = torch.tensor([m])\n",
        "\n",
        "# Check if the number of frequencies and parameter satisfies the Nyquist-Criterion\n",
        "if L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2)):\n",
        "    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\n",
        "else:\n",
        "    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n",
        "\n",
        "# Define the embedding\n",
        "nerf_embedder = SinusoidalEmbedding(\n",
        "    in_channels=1, num_frequencies=L, embedding_type=\"nerf\"\n",
        ").to(device)\n",
        "\n",
        "# Apply amplitude modulation: multiply embedding by parameter after computation\n",
        "# This scales all embedding components by the parameter m\n",
        "nerf_embedding = nerf_embedder(x_normalized).permute(1, 0) * m_tensor\n",
        "\n",
        "# Plot the embedding\n",
        "colors = plt.cm.tab10.colors\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for freq_idx in range(L):\n",
        "    color = colors[freq_idx % len(colors)]\n",
        "    sin_idx = 2 * freq_idx\n",
        "    cos_idx = 2 * freq_idx + 1\n",
        "\n",
        "    plt.plot(x, nerf_embedding[sin_idx], color=color, label=f\"Channel {freq_idx}\")\n",
        "    plt.plot(x, nerf_embedding[cos_idx], color=color)\n",
        "\n",
        "plt.xlabel(\"x\", fontsize=16)\n",
        "plt.ylabel(\"Embedding\", fontsize=16)\n",
        "plt.title(\"NeRF-style embedding with amplitude modulation\", fontsize=18, pad=20)\n",
        "plt.legend(loc=\"lower left\", framealpha=1.0, fontsize=14)\n",
        "plt.locator_params(axis=\"y\", nbins=5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Application to Fourier Neural Operators (FNOs)\n",
        "Fourier Neural Operators (FNOs) learn mappings between functions by operating in the frequency domain.\n",
        "They use the Fourier transform to express data as combinations of sine and cosine components,\n",
        "enabling them to capture complex, multi-scale interactions across frequencies.\n",
        "Given that sinusoidal embeddings also lift low-dimensional data into a richer spectral\n",
        "representation, they complement FNOs naturally.\n",
        "This synergy makes sinusoidal embeddings particularly effective for neural operator architectures.\n",
        "\n",
        "In the general setting for neural operators, we strongly recommend choosing the number of frequencies\n",
        "$L$ such that the Nyquist-Criterion is not violated. This can be done by following the guidelines\n",
        "we provided earlier for selecting $L$ in both transformer-style and NeRF-style embeddings.\n",
        "\n",
        "When dealing with FNOs with a specified number of Fourier modes, $n_{\\text{modes}}$, the\n",
        "highest embedded frequency should ideally also remain below $n_{\\text{modes}}$,\n",
        "as higher frequencies will be zeroed out and not acted upon by the spectral convolution operation.\n",
        "\n",
        "For the NeRF-style embedding, this condition leads to an explicit upper bound on $L$:\n",
        "\n",
        "$$2^{L-1} < n_{\\text{modes}} \\ \\ \\implies \\ \\  L < 1 + \\log_2\\left(n_{\\text{modes}}\\right).$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Setup in Higher Dimensions\n",
        "Let $X \\in \\mathbb{R}^{d \\times N}$ denote a $d$-dimensional input domain consisting of\n",
        "$N$ discretised points, where each row $\\vec{x}_{i} \\in \\mathbb{R}^N$ corresponds to\n",
        "the sampled coordinates along the $i$-th spatial or temporal dimension. Thus, each column\n",
        "of $X$ represents a single point $\\vec{x}_{:,j} \\in \\mathbb{R}^d$ in the\n",
        "$d$-dimensional domain.\n",
        "\n",
        "Building on the 1D embedding function $g$ introduced earlier, we define the\n",
        "multi-dimensional embedding\n",
        "\n",
        "$$h: \\mathbb{R}^{d \\times N} \\ \\  \\rightarrow \\ \\ \\mathbb{R}^{N \\times 2 L d}, \\quad h(X)=\\operatorname{concat}\\left(g\\left(\\vec{x}_1\\right), g\\left(\\vec{x}_2\\right), \\ldots, g\\left(\\vec{x}_d\\right)\\right),$$\n",
        "\n",
        "where each $\\vec{x}_i$ denotes the sampled domain along the $i$-th input dimension.\n",
        "\n",
        "The multi-dimensional embedding function $h$ applies the 1D embedding function $g$\n",
        "independently to each coordinate dimension and concatenates the resulting embeddings\n",
        "along the feature axis. This approach allows the model to capture\n",
        "frequency patterns along each dimension separately while maintaining the overall structure.\n",
        "\n",
        "\n",
        "Below, we include an example of using the `SinusoidalEmbedding` class to construct NeRF-style\n",
        "embeddings for a 3D input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a 1D spatial domain and construct 3D input by repeating the normalized 1D domain\n",
        "dim = 3\n",
        "x_1d = torch.linspace(0, 1, 1000)\n",
        "# Normalize to [0, 2π] and add channel dimension\n",
        "x_normalized_1d = torch.linspace(0, 2 * torch.pi, x_1d.size(0), device=x_1d.device).unsqueeze(1)\n",
        "# Repeat for 3D input: shape (N, 3)\n",
        "x_normalized = x_normalized_1d.repeat(1, dim)\n",
        "\n",
        "# Define the number of frequencies\n",
        "L = 3\n",
        "\n",
        "# Check if the number of frequencies satisfies the Nyquist-Criterion\n",
        "# For multi-dimensional inputs, the constraint applies to each dimension independently\n",
        "if L <= 1 + torch.log2(torch.tensor(len(x_normalized)/2)):\n",
        "    print(f\"Nyquist-Shannon sampling theorem is satisfied for the given number of frequencies {L}.\")\n",
        "else:\n",
        "    print(f\"Nyquist-Shannon sampling theorem is violated for the given number of frequencies {L}.\")\n",
        "\n",
        "# Define the transformer embedding\n",
        "max_positions = 1000\n",
        "transformer_embedder = SinusoidalEmbedding(\n",
        "    in_channels=3,\n",
        "    num_frequencies=L,\n",
        "    embedding_type=\"transformer\",\n",
        "    max_positions=max_positions,\n",
        ").to(device)\n",
        "\n",
        "# Apply transformer-style embedding\n",
        "transformer_embedding = transformer_embedder(x_normalized).permute(1, 0)\n",
        "\n",
        "# Define the NeRF embedding\n",
        "nerf_embedder = SinusoidalEmbedding(\n",
        "    in_channels=dim, num_frequencies=L, embedding_type=\"nerf\"\n",
        ").to(device)\n",
        "\n",
        "# Apply NeRF-style embedding\n",
        "nerf_embedding = nerf_embedder(x_normalized).permute(1, 0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
